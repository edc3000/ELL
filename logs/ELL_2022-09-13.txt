[2022-09-13 14:44:04] - INFO:  ### 将当前配置打印到日志文件中 
[2022-09-13 14:44:04] - INFO: ### project_dir = /Users/liangyan/PycharmProjects/ELL
[2022-09-13 14:44:04] - INFO: ### dataset_dir = /Users/liangyan/PycharmProjects/ELL/data
[2022-09-13 14:44:04] - INFO: ### pretrained_model_dir = /Users/liangyan/PycharmProjects/ELL/bert_base_uncased
[2022-09-13 14:44:04] - INFO: ### vocab_path = /Users/liangyan/PycharmProjects/ELL/bert_base_uncased/vocab.txt
[2022-09-13 14:44:04] - INFO: ### device = cpu
[2022-09-13 14:44:04] - INFO: ### train_file_path = /Users/liangyan/PycharmProjects/ELL/data/train.csv
[2022-09-13 14:44:04] - INFO: ### test_file_path = /Users/liangyan/PycharmProjects/ELL/data/test.csv
[2022-09-13 14:44:04] - INFO: ### sub_file_path = /Users/liangyan/PycharmProjects/ELL/data/sample_submission.csv
[2022-09-13 14:44:04] - INFO: ### model_save_dir = /Users/liangyan/PycharmProjects/ELL/cache
[2022-09-13 14:44:04] - INFO: ### logs_save_dir = /Users/liangyan/PycharmProjects/ELL/logs
[2022-09-13 14:44:04] - INFO: ### batch_size = 10
[2022-09-13 14:44:04] - INFO: ### learning_rate = 3.5e-05
[2022-09-13 14:44:04] - INFO: ### epochs = 1
[2022-09-13 14:44:04] - INFO: ### nums_labels = 6
[2022-09-13 14:44:04] - INFO: ### labels = ['cohesion', 'syntax', 'vocabulary', 'phraseology', 'grammar', 'conventions']
[2022-09-13 14:44:04] - INFO: ### return_dict = True
[2022-09-13 14:44:04] - INFO: ### output_hidden_states = False
[2022-09-13 14:44:04] - INFO: ### output_attentions = False
[2022-09-13 14:44:04] - INFO: ### torchscript = False
[2022-09-13 14:44:04] - INFO: ### torch_dtype = None
[2022-09-13 14:44:04] - INFO: ### use_bfloat16 = False
[2022-09-13 14:44:04] - INFO: ### pruned_heads = {}
[2022-09-13 14:44:04] - INFO: ### tie_word_embeddings = True
[2022-09-13 14:44:04] - INFO: ### is_encoder_decoder = False
[2022-09-13 14:44:04] - INFO: ### is_decoder = False
[2022-09-13 14:44:04] - INFO: ### cross_attention_hidden_size = None
[2022-09-13 14:44:04] - INFO: ### add_cross_attention = False
[2022-09-13 14:44:04] - INFO: ### tie_encoder_decoder = False
[2022-09-13 14:44:04] - INFO: ### max_length = 20
[2022-09-13 14:44:04] - INFO: ### min_length = 0
[2022-09-13 14:44:04] - INFO: ### do_sample = False
[2022-09-13 14:44:04] - INFO: ### early_stopping = False
[2022-09-13 14:44:04] - INFO: ### num_beams = 1
[2022-09-13 14:44:04] - INFO: ### num_beam_groups = 1
[2022-09-13 14:44:04] - INFO: ### diversity_penalty = 0.0
[2022-09-13 14:44:04] - INFO: ### temperature = 1.0
[2022-09-13 14:44:04] - INFO: ### top_k = 50
[2022-09-13 14:44:04] - INFO: ### top_p = 1.0
[2022-09-13 14:44:04] - INFO: ### typical_p = 1.0
[2022-09-13 14:44:04] - INFO: ### repetition_penalty = 1.0
[2022-09-13 14:44:04] - INFO: ### length_penalty = 1.0
[2022-09-13 14:44:04] - INFO: ### no_repeat_ngram_size = 0
[2022-09-13 14:44:04] - INFO: ### encoder_no_repeat_ngram_size = 0
[2022-09-13 14:44:04] - INFO: ### bad_words_ids = None
[2022-09-13 14:44:04] - INFO: ### num_return_sequences = 1
[2022-09-13 14:44:04] - INFO: ### chunk_size_feed_forward = 0
[2022-09-13 14:44:04] - INFO: ### output_scores = False
[2022-09-13 14:44:04] - INFO: ### return_dict_in_generate = False
[2022-09-13 14:44:04] - INFO: ### forced_bos_token_id = None
[2022-09-13 14:44:04] - INFO: ### forced_eos_token_id = None
[2022-09-13 14:44:04] - INFO: ### remove_invalid_values = False
[2022-09-13 14:44:04] - INFO: ### exponential_decay_length_penalty = None
[2022-09-13 14:44:04] - INFO: ### architectures = ['BertForMaskedLM']
[2022-09-13 14:44:04] - INFO: ### finetuning_task = None
[2022-09-13 14:44:04] - INFO: ### id2label = {0: 'LABEL_0', 1: 'LABEL_1'}
[2022-09-13 14:44:04] - INFO: ### label2id = {'LABEL_0': 0, 'LABEL_1': 1}
[2022-09-13 14:44:04] - INFO: ### tokenizer_class = None
[2022-09-13 14:44:04] - INFO: ### prefix = None
[2022-09-13 14:44:04] - INFO: ### bos_token_id = None
[2022-09-13 14:44:04] - INFO: ### pad_token_id = 0
[2022-09-13 14:44:04] - INFO: ### eos_token_id = None
[2022-09-13 14:44:04] - INFO: ### sep_token_id = None
[2022-09-13 14:44:04] - INFO: ### decoder_start_token_id = None
[2022-09-13 14:44:04] - INFO: ### task_specific_params = None
[2022-09-13 14:44:04] - INFO: ### problem_type = None
[2022-09-13 14:44:04] - INFO: ### _name_or_path = 
[2022-09-13 14:44:04] - INFO: ### transformers_version = 4.6.0.dev0
[2022-09-13 14:44:04] - INFO: ### gradient_checkpointing = False
[2022-09-13 14:44:04] - INFO: ### model_type = bert
[2022-09-13 14:44:04] - INFO: ### vocab_size = 30522
[2022-09-13 14:44:04] - INFO: ### hidden_size = 768
[2022-09-13 14:44:04] - INFO: ### num_hidden_layers = 12
[2022-09-13 14:44:04] - INFO: ### num_attention_heads = 12
[2022-09-13 14:44:04] - INFO: ### hidden_act = gelu
[2022-09-13 14:44:04] - INFO: ### intermediate_size = 3072
[2022-09-13 14:44:04] - INFO: ### hidden_dropout_prob = 0.1
[2022-09-13 14:44:04] - INFO: ### attention_probs_dropout_prob = 0.1
[2022-09-13 14:44:04] - INFO: ### max_position_embeddings = 512
[2022-09-13 14:44:04] - INFO: ### type_vocab_size = 2
[2022-09-13 14:44:04] - INFO: ### initializer_range = 0.02
[2022-09-13 14:44:04] - INFO: ### layer_norm_eps = 1e-12
[2022-09-13 14:44:04] - INFO: ### position_embedding_type = absolute
[2022-09-13 14:44:04] - INFO: ### use_cache = True
[2022-09-13 14:44:04] - INFO: ### classifier_dropout = None
[2022-09-13 14:44:08] - INFO: tensor([[-0.0413, -0.2220, -0.1603,  0.0753,  0.0620, -0.4372],
        [ 0.0723, -0.1215, -0.0824, -0.0785,  0.1239, -0.3978]])
[2022-09-13 14:44:09] - INFO: tensor([[ 0.0504, -0.2122, -0.0899,  0.1254,  0.0270, -0.4201]])
[2022-09-13 14:44:09] - INFO: [[1. 1. 1. 1. 1. 1.]
 [1. 1. 1. 1. 1. 1.]
 [1. 1. 1. 1. 1. 1.]]
[2022-09-13 14:46:30] - INFO:  ### 将当前配置打印到日志文件中 
[2022-09-13 14:46:30] - INFO: ### project_dir = /Users/liangyan/PycharmProjects/ELL
[2022-09-13 14:46:30] - INFO: ### dataset_dir = /Users/liangyan/PycharmProjects/ELL/data
[2022-09-13 14:46:30] - INFO: ### pretrained_model_dir = /Users/liangyan/PycharmProjects/ELL/bert_base_uncased
[2022-09-13 14:46:30] - INFO: ### vocab_path = /Users/liangyan/PycharmProjects/ELL/bert_base_uncased/vocab.txt
[2022-09-13 14:46:30] - INFO: ### device = cpu
[2022-09-13 14:46:30] - INFO: ### train_file_path = /Users/liangyan/PycharmProjects/ELL/data/train.csv
[2022-09-13 14:46:30] - INFO: ### test_file_path = /Users/liangyan/PycharmProjects/ELL/data/test.csv
[2022-09-13 14:46:30] - INFO: ### sub_file_path = /Users/liangyan/PycharmProjects/ELL/data/sample_submission.csv
[2022-09-13 14:46:30] - INFO: ### model_save_dir = /Users/liangyan/PycharmProjects/ELL/cache
[2022-09-13 14:46:30] - INFO: ### logs_save_dir = /Users/liangyan/PycharmProjects/ELL/logs
[2022-09-13 14:46:30] - INFO: ### batch_size = 10
[2022-09-13 14:46:30] - INFO: ### learning_rate = 3.5e-05
[2022-09-13 14:46:30] - INFO: ### epochs = 1
[2022-09-13 14:46:30] - INFO: ### nums_labels = 6
[2022-09-13 14:46:30] - INFO: ### labels = ['cohesion', 'syntax', 'vocabulary', 'phraseology', 'grammar', 'conventions']
[2022-09-13 14:46:30] - INFO: ### return_dict = True
[2022-09-13 14:46:30] - INFO: ### output_hidden_states = False
[2022-09-13 14:46:30] - INFO: ### output_attentions = False
[2022-09-13 14:46:30] - INFO: ### torchscript = False
[2022-09-13 14:46:30] - INFO: ### torch_dtype = None
[2022-09-13 14:46:30] - INFO: ### use_bfloat16 = False
[2022-09-13 14:46:30] - INFO: ### pruned_heads = {}
[2022-09-13 14:46:30] - INFO: ### tie_word_embeddings = True
[2022-09-13 14:46:30] - INFO: ### is_encoder_decoder = False
[2022-09-13 14:46:30] - INFO: ### is_decoder = False
[2022-09-13 14:46:30] - INFO: ### cross_attention_hidden_size = None
[2022-09-13 14:46:30] - INFO: ### add_cross_attention = False
[2022-09-13 14:46:30] - INFO: ### tie_encoder_decoder = False
[2022-09-13 14:46:30] - INFO: ### max_length = 20
[2022-09-13 14:46:30] - INFO: ### min_length = 0
[2022-09-13 14:46:30] - INFO: ### do_sample = False
[2022-09-13 14:46:30] - INFO: ### early_stopping = False
[2022-09-13 14:46:30] - INFO: ### num_beams = 1
[2022-09-13 14:46:30] - INFO: ### num_beam_groups = 1
[2022-09-13 14:46:30] - INFO: ### diversity_penalty = 0.0
[2022-09-13 14:46:30] - INFO: ### temperature = 1.0
[2022-09-13 14:46:30] - INFO: ### top_k = 50
[2022-09-13 14:46:30] - INFO: ### top_p = 1.0
[2022-09-13 14:46:30] - INFO: ### typical_p = 1.0
[2022-09-13 14:46:30] - INFO: ### repetition_penalty = 1.0
[2022-09-13 14:46:30] - INFO: ### length_penalty = 1.0
[2022-09-13 14:46:30] - INFO: ### no_repeat_ngram_size = 0
[2022-09-13 14:46:30] - INFO: ### encoder_no_repeat_ngram_size = 0
[2022-09-13 14:46:30] - INFO: ### bad_words_ids = None
[2022-09-13 14:46:30] - INFO: ### num_return_sequences = 1
[2022-09-13 14:46:30] - INFO: ### chunk_size_feed_forward = 0
[2022-09-13 14:46:30] - INFO: ### output_scores = False
[2022-09-13 14:46:30] - INFO: ### return_dict_in_generate = False
[2022-09-13 14:46:30] - INFO: ### forced_bos_token_id = None
[2022-09-13 14:46:30] - INFO: ### forced_eos_token_id = None
[2022-09-13 14:46:30] - INFO: ### remove_invalid_values = False
[2022-09-13 14:46:30] - INFO: ### exponential_decay_length_penalty = None
[2022-09-13 14:46:30] - INFO: ### architectures = ['BertForMaskedLM']
[2022-09-13 14:46:30] - INFO: ### finetuning_task = None
[2022-09-13 14:46:30] - INFO: ### id2label = {0: 'LABEL_0', 1: 'LABEL_1'}
[2022-09-13 14:46:30] - INFO: ### label2id = {'LABEL_0': 0, 'LABEL_1': 1}
[2022-09-13 14:46:30] - INFO: ### tokenizer_class = None
[2022-09-13 14:46:30] - INFO: ### prefix = None
[2022-09-13 14:46:30] - INFO: ### bos_token_id = None
[2022-09-13 14:46:30] - INFO: ### pad_token_id = 0
[2022-09-13 14:46:30] - INFO: ### eos_token_id = None
[2022-09-13 14:46:30] - INFO: ### sep_token_id = None
[2022-09-13 14:46:30] - INFO: ### decoder_start_token_id = None
[2022-09-13 14:46:30] - INFO: ### task_specific_params = None
[2022-09-13 14:46:30] - INFO: ### problem_type = None
[2022-09-13 14:46:30] - INFO: ### _name_or_path = 
[2022-09-13 14:46:30] - INFO: ### transformers_version = 4.6.0.dev0
[2022-09-13 14:46:30] - INFO: ### gradient_checkpointing = False
[2022-09-13 14:46:30] - INFO: ### model_type = bert
[2022-09-13 14:46:30] - INFO: ### vocab_size = 30522
[2022-09-13 14:46:30] - INFO: ### hidden_size = 768
[2022-09-13 14:46:30] - INFO: ### num_hidden_layers = 12
[2022-09-13 14:46:30] - INFO: ### num_attention_heads = 12
[2022-09-13 14:46:30] - INFO: ### hidden_act = gelu
[2022-09-13 14:46:30] - INFO: ### intermediate_size = 3072
[2022-09-13 14:46:30] - INFO: ### hidden_dropout_prob = 0.1
[2022-09-13 14:46:30] - INFO: ### attention_probs_dropout_prob = 0.1
[2022-09-13 14:46:30] - INFO: ### max_position_embeddings = 512
[2022-09-13 14:46:30] - INFO: ### type_vocab_size = 2
[2022-09-13 14:46:30] - INFO: ### initializer_range = 0.02
[2022-09-13 14:46:30] - INFO: ### layer_norm_eps = 1e-12
[2022-09-13 14:46:30] - INFO: ### position_embedding_type = absolute
[2022-09-13 14:46:30] - INFO: ### use_cache = True
[2022-09-13 14:46:30] - INFO: ### classifier_dropout = None
[2022-09-13 14:51:23] - INFO:  ### 将当前配置打印到日志文件中 
[2022-09-13 14:51:23] - INFO: ### project_dir = /Users/liangyan/PycharmProjects/ELL
[2022-09-13 14:51:23] - INFO: ### dataset_dir = /Users/liangyan/PycharmProjects/ELL/data
[2022-09-13 14:51:23] - INFO: ### pretrained_model_dir = /Users/liangyan/PycharmProjects/ELL/bert_base_uncased
[2022-09-13 14:51:23] - INFO: ### vocab_path = /Users/liangyan/PycharmProjects/ELL/bert_base_uncased/vocab.txt
[2022-09-13 14:51:23] - INFO: ### device = cpu
[2022-09-13 14:51:23] - INFO: ### train_file_path = /Users/liangyan/PycharmProjects/ELL/data/train.csv
[2022-09-13 14:51:23] - INFO: ### test_file_path = /Users/liangyan/PycharmProjects/ELL/data/test.csv
[2022-09-13 14:51:23] - INFO: ### sub_file_path = /Users/liangyan/PycharmProjects/ELL/data/sample_submission.csv
[2022-09-13 14:51:23] - INFO: ### model_save_dir = /Users/liangyan/PycharmProjects/ELL/cache
[2022-09-13 14:51:23] - INFO: ### logs_save_dir = /Users/liangyan/PycharmProjects/ELL/logs
[2022-09-13 14:51:23] - INFO: ### batch_size = 10
[2022-09-13 14:51:23] - INFO: ### learning_rate = 3.5e-05
[2022-09-13 14:51:23] - INFO: ### epochs = 1
[2022-09-13 14:51:23] - INFO: ### nums_labels = 6
[2022-09-13 14:51:23] - INFO: ### labels = ['cohesion', 'syntax', 'vocabulary', 'phraseology', 'grammar', 'conventions']
[2022-09-13 14:51:23] - INFO: ### return_dict = True
[2022-09-13 14:51:23] - INFO: ### output_hidden_states = False
[2022-09-13 14:51:23] - INFO: ### output_attentions = False
[2022-09-13 14:51:23] - INFO: ### torchscript = False
[2022-09-13 14:51:23] - INFO: ### torch_dtype = None
[2022-09-13 14:51:23] - INFO: ### use_bfloat16 = False
[2022-09-13 14:51:23] - INFO: ### pruned_heads = {}
[2022-09-13 14:51:23] - INFO: ### tie_word_embeddings = True
[2022-09-13 14:51:23] - INFO: ### is_encoder_decoder = False
[2022-09-13 14:51:23] - INFO: ### is_decoder = False
[2022-09-13 14:51:23] - INFO: ### cross_attention_hidden_size = None
[2022-09-13 14:51:23] - INFO: ### add_cross_attention = False
[2022-09-13 14:51:23] - INFO: ### tie_encoder_decoder = False
[2022-09-13 14:51:23] - INFO: ### max_length = 20
[2022-09-13 14:51:23] - INFO: ### min_length = 0
[2022-09-13 14:51:23] - INFO: ### do_sample = False
[2022-09-13 14:51:23] - INFO: ### early_stopping = False
[2022-09-13 14:51:23] - INFO: ### num_beams = 1
[2022-09-13 14:51:23] - INFO: ### num_beam_groups = 1
[2022-09-13 14:51:23] - INFO: ### diversity_penalty = 0.0
[2022-09-13 14:51:23] - INFO: ### temperature = 1.0
[2022-09-13 14:51:23] - INFO: ### top_k = 50
[2022-09-13 14:51:23] - INFO: ### top_p = 1.0
[2022-09-13 14:51:23] - INFO: ### typical_p = 1.0
[2022-09-13 14:51:23] - INFO: ### repetition_penalty = 1.0
[2022-09-13 14:51:23] - INFO: ### length_penalty = 1.0
[2022-09-13 14:51:23] - INFO: ### no_repeat_ngram_size = 0
[2022-09-13 14:51:23] - INFO: ### encoder_no_repeat_ngram_size = 0
[2022-09-13 14:51:23] - INFO: ### bad_words_ids = None
[2022-09-13 14:51:23] - INFO: ### num_return_sequences = 1
[2022-09-13 14:51:23] - INFO: ### chunk_size_feed_forward = 0
[2022-09-13 14:51:23] - INFO: ### output_scores = False
[2022-09-13 14:51:23] - INFO: ### return_dict_in_generate = False
[2022-09-13 14:51:23] - INFO: ### forced_bos_token_id = None
[2022-09-13 14:51:23] - INFO: ### forced_eos_token_id = None
[2022-09-13 14:51:23] - INFO: ### remove_invalid_values = False
[2022-09-13 14:51:23] - INFO: ### exponential_decay_length_penalty = None
[2022-09-13 14:51:23] - INFO: ### architectures = ['BertForMaskedLM']
[2022-09-13 14:51:23] - INFO: ### finetuning_task = None
[2022-09-13 14:51:23] - INFO: ### id2label = {0: 'LABEL_0', 1: 'LABEL_1'}
[2022-09-13 14:51:23] - INFO: ### label2id = {'LABEL_0': 0, 'LABEL_1': 1}
[2022-09-13 14:51:23] - INFO: ### tokenizer_class = None
[2022-09-13 14:51:23] - INFO: ### prefix = None
[2022-09-13 14:51:23] - INFO: ### bos_token_id = None
[2022-09-13 14:51:23] - INFO: ### pad_token_id = 0
[2022-09-13 14:51:23] - INFO: ### eos_token_id = None
[2022-09-13 14:51:23] - INFO: ### sep_token_id = None
[2022-09-13 14:51:23] - INFO: ### decoder_start_token_id = None
[2022-09-13 14:51:23] - INFO: ### task_specific_params = None
[2022-09-13 14:51:23] - INFO: ### problem_type = None
[2022-09-13 14:51:23] - INFO: ### _name_or_path = 
[2022-09-13 14:51:23] - INFO: ### transformers_version = 4.6.0.dev0
[2022-09-13 14:51:23] - INFO: ### gradient_checkpointing = False
[2022-09-13 14:51:23] - INFO: ### model_type = bert
[2022-09-13 14:51:23] - INFO: ### vocab_size = 30522
[2022-09-13 14:51:23] - INFO: ### hidden_size = 768
[2022-09-13 14:51:23] - INFO: ### num_hidden_layers = 12
[2022-09-13 14:51:23] - INFO: ### num_attention_heads = 12
[2022-09-13 14:51:23] - INFO: ### hidden_act = gelu
[2022-09-13 14:51:23] - INFO: ### intermediate_size = 3072
[2022-09-13 14:51:23] - INFO: ### hidden_dropout_prob = 0.1
[2022-09-13 14:51:23] - INFO: ### attention_probs_dropout_prob = 0.1
[2022-09-13 14:51:23] - INFO: ### max_position_embeddings = 512
[2022-09-13 14:51:23] - INFO: ### type_vocab_size = 2
[2022-09-13 14:51:23] - INFO: ### initializer_range = 0.02
[2022-09-13 14:51:23] - INFO: ### layer_norm_eps = 1e-12
[2022-09-13 14:51:23] - INFO: ### position_embedding_type = absolute
[2022-09-13 14:51:23] - INFO: ### use_cache = True
[2022-09-13 14:51:23] - INFO: ### classifier_dropout = None
[2022-09-13 14:56:06] - INFO:  ### 将当前配置打印到日志文件中 
[2022-09-13 14:56:06] - INFO: ### project_dir = /Users/liangyan/PycharmProjects/ELL
[2022-09-13 14:56:06] - INFO: ### dataset_dir = /Users/liangyan/PycharmProjects/ELL/data
[2022-09-13 14:56:06] - INFO: ### pretrained_model_dir = /Users/liangyan/PycharmProjects/ELL/bert_base_uncased
[2022-09-13 14:56:06] - INFO: ### vocab_path = /Users/liangyan/PycharmProjects/ELL/bert_base_uncased/vocab.txt
[2022-09-13 14:56:06] - INFO: ### device = cpu
[2022-09-13 14:56:06] - INFO: ### train_file_path = /Users/liangyan/PycharmProjects/ELL/data/train.csv
[2022-09-13 14:56:06] - INFO: ### test_file_path = /Users/liangyan/PycharmProjects/ELL/data/test.csv
[2022-09-13 14:56:06] - INFO: ### sub_file_path = /Users/liangyan/PycharmProjects/ELL/data/sample_submission.csv
[2022-09-13 14:56:06] - INFO: ### model_save_dir = /Users/liangyan/PycharmProjects/ELL/cache
[2022-09-13 14:56:06] - INFO: ### logs_save_dir = /Users/liangyan/PycharmProjects/ELL/logs
[2022-09-13 14:56:06] - INFO: ### batch_size = 10
[2022-09-13 14:56:06] - INFO: ### learning_rate = 3.5e-05
[2022-09-13 14:56:06] - INFO: ### epochs = 1
[2022-09-13 14:56:06] - INFO: ### nums_labels = 6
[2022-09-13 14:56:06] - INFO: ### labels = ['cohesion', 'syntax', 'vocabulary', 'phraseology', 'grammar', 'conventions']
[2022-09-13 14:56:06] - INFO: ### return_dict = True
[2022-09-13 14:56:06] - INFO: ### output_hidden_states = False
[2022-09-13 14:56:06] - INFO: ### output_attentions = False
[2022-09-13 14:56:06] - INFO: ### torchscript = False
[2022-09-13 14:56:06] - INFO: ### torch_dtype = None
[2022-09-13 14:56:06] - INFO: ### use_bfloat16 = False
[2022-09-13 14:56:06] - INFO: ### pruned_heads = {}
[2022-09-13 14:56:06] - INFO: ### tie_word_embeddings = True
[2022-09-13 14:56:06] - INFO: ### is_encoder_decoder = False
[2022-09-13 14:56:06] - INFO: ### is_decoder = False
[2022-09-13 14:56:06] - INFO: ### cross_attention_hidden_size = None
[2022-09-13 14:56:06] - INFO: ### add_cross_attention = False
[2022-09-13 14:56:06] - INFO: ### tie_encoder_decoder = False
[2022-09-13 14:56:06] - INFO: ### max_length = 20
[2022-09-13 14:56:06] - INFO: ### min_length = 0
[2022-09-13 14:56:06] - INFO: ### do_sample = False
[2022-09-13 14:56:06] - INFO: ### early_stopping = False
[2022-09-13 14:56:06] - INFO: ### num_beams = 1
[2022-09-13 14:56:06] - INFO: ### num_beam_groups = 1
[2022-09-13 14:56:06] - INFO: ### diversity_penalty = 0.0
[2022-09-13 14:56:06] - INFO: ### temperature = 1.0
[2022-09-13 14:56:06] - INFO: ### top_k = 50
[2022-09-13 14:56:06] - INFO: ### top_p = 1.0
[2022-09-13 14:56:06] - INFO: ### typical_p = 1.0
[2022-09-13 14:56:06] - INFO: ### repetition_penalty = 1.0
[2022-09-13 14:56:06] - INFO: ### length_penalty = 1.0
[2022-09-13 14:56:06] - INFO: ### no_repeat_ngram_size = 0
[2022-09-13 14:56:06] - INFO: ### encoder_no_repeat_ngram_size = 0
[2022-09-13 14:56:06] - INFO: ### bad_words_ids = None
[2022-09-13 14:56:06] - INFO: ### num_return_sequences = 1
[2022-09-13 14:56:06] - INFO: ### chunk_size_feed_forward = 0
[2022-09-13 14:56:06] - INFO: ### output_scores = False
[2022-09-13 14:56:06] - INFO: ### return_dict_in_generate = False
[2022-09-13 14:56:06] - INFO: ### forced_bos_token_id = None
[2022-09-13 14:56:06] - INFO: ### forced_eos_token_id = None
[2022-09-13 14:56:06] - INFO: ### remove_invalid_values = False
[2022-09-13 14:56:06] - INFO: ### exponential_decay_length_penalty = None
[2022-09-13 14:56:06] - INFO: ### architectures = ['BertForMaskedLM']
[2022-09-13 14:56:06] - INFO: ### finetuning_task = None
[2022-09-13 14:56:06] - INFO: ### id2label = {0: 'LABEL_0', 1: 'LABEL_1'}
[2022-09-13 14:56:06] - INFO: ### label2id = {'LABEL_0': 0, 'LABEL_1': 1}
[2022-09-13 14:56:06] - INFO: ### tokenizer_class = None
[2022-09-13 14:56:06] - INFO: ### prefix = None
[2022-09-13 14:56:06] - INFO: ### bos_token_id = None
[2022-09-13 14:56:06] - INFO: ### pad_token_id = 0
[2022-09-13 14:56:06] - INFO: ### eos_token_id = None
[2022-09-13 14:56:06] - INFO: ### sep_token_id = None
[2022-09-13 14:56:06] - INFO: ### decoder_start_token_id = None
[2022-09-13 14:56:06] - INFO: ### task_specific_params = None
[2022-09-13 14:56:06] - INFO: ### problem_type = None
[2022-09-13 14:56:06] - INFO: ### _name_or_path = 
[2022-09-13 14:56:06] - INFO: ### transformers_version = 4.6.0.dev0
[2022-09-13 14:56:06] - INFO: ### gradient_checkpointing = False
[2022-09-13 14:56:06] - INFO: ### model_type = bert
[2022-09-13 14:56:06] - INFO: ### vocab_size = 30522
[2022-09-13 14:56:06] - INFO: ### hidden_size = 768
[2022-09-13 14:56:06] - INFO: ### num_hidden_layers = 12
[2022-09-13 14:56:06] - INFO: ### num_attention_heads = 12
[2022-09-13 14:56:06] - INFO: ### hidden_act = gelu
[2022-09-13 14:56:06] - INFO: ### intermediate_size = 3072
[2022-09-13 14:56:06] - INFO: ### hidden_dropout_prob = 0.1
[2022-09-13 14:56:06] - INFO: ### attention_probs_dropout_prob = 0.1
[2022-09-13 14:56:06] - INFO: ### max_position_embeddings = 512
[2022-09-13 14:56:06] - INFO: ### type_vocab_size = 2
[2022-09-13 14:56:06] - INFO: ### initializer_range = 0.02
[2022-09-13 14:56:06] - INFO: ### layer_norm_eps = 1e-12
[2022-09-13 14:56:06] - INFO: ### position_embedding_type = absolute
[2022-09-13 14:56:06] - INFO: ### use_cache = True
[2022-09-13 14:56:06] - INFO: ### classifier_dropout = None
[2022-09-13 14:56:08] - INFO:         text_id  ... conventions
0  0016926B079C  ...         3.0
1  0022683E9EA5  ...         2.5
2  00299B378633  ...         2.5
3  003885A45F42  ...         5.0
4  0049B1DF5CCC  ...         2.5

[5 rows x 8 columns]
[2022-09-13 15:05:10] - INFO:  ### 将当前配置打印到日志文件中 
[2022-09-13 15:05:10] - INFO: ### project_dir = /Users/liangyan/PycharmProjects/ELL
[2022-09-13 15:05:10] - INFO: ### dataset_dir = /Users/liangyan/PycharmProjects/ELL/data
[2022-09-13 15:05:10] - INFO: ### pretrained_model_dir = /Users/liangyan/PycharmProjects/ELL/bert_base_uncased
[2022-09-13 15:05:10] - INFO: ### vocab_path = /Users/liangyan/PycharmProjects/ELL/bert_base_uncased/vocab.txt
[2022-09-13 15:05:10] - INFO: ### device = cpu
[2022-09-13 15:05:10] - INFO: ### train_file_path = /Users/liangyan/PycharmProjects/ELL/data/train.csv
[2022-09-13 15:05:10] - INFO: ### test_file_path = /Users/liangyan/PycharmProjects/ELL/data/test.csv
[2022-09-13 15:05:10] - INFO: ### sub_file_path = /Users/liangyan/PycharmProjects/ELL/data/sample_submission.csv
[2022-09-13 15:05:10] - INFO: ### model_save_dir = /Users/liangyan/PycharmProjects/ELL/cache
[2022-09-13 15:05:10] - INFO: ### logs_save_dir = /Users/liangyan/PycharmProjects/ELL/logs
[2022-09-13 15:05:10] - INFO: ### batch_size = 20
[2022-09-13 15:05:10] - INFO: ### learning_rate = 3.5e-05
[2022-09-13 15:05:10] - INFO: ### epochs = 1
[2022-09-13 15:05:10] - INFO: ### nums_labels = 6
[2022-09-13 15:05:10] - INFO: ### labels = ['cohesion', 'syntax', 'vocabulary', 'phraseology', 'grammar', 'conventions']
[2022-09-13 15:05:10] - INFO: ### return_dict = True
[2022-09-13 15:05:10] - INFO: ### output_hidden_states = False
[2022-09-13 15:05:10] - INFO: ### output_attentions = False
[2022-09-13 15:05:10] - INFO: ### torchscript = False
[2022-09-13 15:05:10] - INFO: ### torch_dtype = None
[2022-09-13 15:05:10] - INFO: ### use_bfloat16 = False
[2022-09-13 15:05:10] - INFO: ### pruned_heads = {}
[2022-09-13 15:05:10] - INFO: ### tie_word_embeddings = True
[2022-09-13 15:05:10] - INFO: ### is_encoder_decoder = False
[2022-09-13 15:05:10] - INFO: ### is_decoder = False
[2022-09-13 15:05:10] - INFO: ### cross_attention_hidden_size = None
[2022-09-13 15:05:10] - INFO: ### add_cross_attention = False
[2022-09-13 15:05:10] - INFO: ### tie_encoder_decoder = False
[2022-09-13 15:05:10] - INFO: ### max_length = 20
[2022-09-13 15:05:10] - INFO: ### min_length = 0
[2022-09-13 15:05:10] - INFO: ### do_sample = False
[2022-09-13 15:05:10] - INFO: ### early_stopping = False
[2022-09-13 15:05:10] - INFO: ### num_beams = 1
[2022-09-13 15:05:10] - INFO: ### num_beam_groups = 1
[2022-09-13 15:05:10] - INFO: ### diversity_penalty = 0.0
[2022-09-13 15:05:10] - INFO: ### temperature = 1.0
[2022-09-13 15:05:10] - INFO: ### top_k = 50
[2022-09-13 15:05:10] - INFO: ### top_p = 1.0
[2022-09-13 15:05:10] - INFO: ### typical_p = 1.0
[2022-09-13 15:05:10] - INFO: ### repetition_penalty = 1.0
[2022-09-13 15:05:10] - INFO: ### length_penalty = 1.0
[2022-09-13 15:05:10] - INFO: ### no_repeat_ngram_size = 0
[2022-09-13 15:05:10] - INFO: ### encoder_no_repeat_ngram_size = 0
[2022-09-13 15:05:10] - INFO: ### bad_words_ids = None
[2022-09-13 15:05:10] - INFO: ### num_return_sequences = 1
[2022-09-13 15:05:10] - INFO: ### chunk_size_feed_forward = 0
[2022-09-13 15:05:10] - INFO: ### output_scores = False
[2022-09-13 15:05:10] - INFO: ### return_dict_in_generate = False
[2022-09-13 15:05:10] - INFO: ### forced_bos_token_id = None
[2022-09-13 15:05:10] - INFO: ### forced_eos_token_id = None
[2022-09-13 15:05:10] - INFO: ### remove_invalid_values = False
[2022-09-13 15:05:10] - INFO: ### exponential_decay_length_penalty = None
[2022-09-13 15:05:10] - INFO: ### architectures = ['BertForMaskedLM']
[2022-09-13 15:05:10] - INFO: ### finetuning_task = None
[2022-09-13 15:05:10] - INFO: ### id2label = {0: 'LABEL_0', 1: 'LABEL_1'}
[2022-09-13 15:05:10] - INFO: ### label2id = {'LABEL_0': 0, 'LABEL_1': 1}
[2022-09-13 15:05:10] - INFO: ### tokenizer_class = None
[2022-09-13 15:05:10] - INFO: ### prefix = None
[2022-09-13 15:05:10] - INFO: ### bos_token_id = None
[2022-09-13 15:05:10] - INFO: ### pad_token_id = 0
[2022-09-13 15:05:10] - INFO: ### eos_token_id = None
[2022-09-13 15:05:10] - INFO: ### sep_token_id = None
[2022-09-13 15:05:10] - INFO: ### decoder_start_token_id = None
[2022-09-13 15:05:10] - INFO: ### task_specific_params = None
[2022-09-13 15:05:10] - INFO: ### problem_type = None
[2022-09-13 15:05:10] - INFO: ### _name_or_path = 
[2022-09-13 15:05:10] - INFO: ### transformers_version = 4.6.0.dev0
[2022-09-13 15:05:10] - INFO: ### gradient_checkpointing = False
[2022-09-13 15:05:10] - INFO: ### model_type = bert
[2022-09-13 15:05:10] - INFO: ### vocab_size = 30522
[2022-09-13 15:05:10] - INFO: ### hidden_size = 768
[2022-09-13 15:05:10] - INFO: ### num_hidden_layers = 12
[2022-09-13 15:05:10] - INFO: ### num_attention_heads = 12
[2022-09-13 15:05:10] - INFO: ### hidden_act = gelu
[2022-09-13 15:05:10] - INFO: ### intermediate_size = 3072
[2022-09-13 15:05:10] - INFO: ### hidden_dropout_prob = 0.1
[2022-09-13 15:05:10] - INFO: ### attention_probs_dropout_prob = 0.1
[2022-09-13 15:05:10] - INFO: ### max_position_embeddings = 512
[2022-09-13 15:05:10] - INFO: ### type_vocab_size = 2
[2022-09-13 15:05:10] - INFO: ### initializer_range = 0.02
[2022-09-13 15:05:10] - INFO: ### layer_norm_eps = 1e-12
[2022-09-13 15:05:10] - INFO: ### position_embedding_type = absolute
[2022-09-13 15:05:10] - INFO: ### use_cache = True
[2022-09-13 15:05:10] - INFO: ### classifier_dropout = None
[2022-09-13 15:05:11] - INFO:         text_id  ... conventions
0  0016926B079C  ...         3.0
1  0022683E9EA5  ...         2.5
2  00299B378633  ...         2.5
3  003885A45F42  ...         5.0
4  0049B1DF5CCC  ...         2.5

[5 rows x 8 columns]
[2022-09-13 15:06:57] - INFO:  ### 将当前配置打印到日志文件中 
[2022-09-13 15:06:57] - INFO: ### project_dir = /Users/liangyan/PycharmProjects/ELL
[2022-09-13 15:06:57] - INFO: ### dataset_dir = /Users/liangyan/PycharmProjects/ELL/data
[2022-09-13 15:06:57] - INFO: ### pretrained_model_dir = /Users/liangyan/PycharmProjects/ELL/bert_base_uncased
[2022-09-13 15:06:57] - INFO: ### vocab_path = /Users/liangyan/PycharmProjects/ELL/bert_base_uncased/vocab.txt
[2022-09-13 15:06:57] - INFO: ### device = cpu
[2022-09-13 15:06:57] - INFO: ### train_file_path = /Users/liangyan/PycharmProjects/ELL/data/train.csv
[2022-09-13 15:06:57] - INFO: ### test_file_path = /Users/liangyan/PycharmProjects/ELL/data/test.csv
[2022-09-13 15:06:57] - INFO: ### sub_file_path = /Users/liangyan/PycharmProjects/ELL/data/sample_submission.csv
[2022-09-13 15:06:57] - INFO: ### model_save_dir = /Users/liangyan/PycharmProjects/ELL/cache
[2022-09-13 15:06:57] - INFO: ### logs_save_dir = /Users/liangyan/PycharmProjects/ELL/logs
[2022-09-13 15:06:57] - INFO: ### batch_size = 20
[2022-09-13 15:06:57] - INFO: ### learning_rate = 3.5e-05
[2022-09-13 15:06:57] - INFO: ### epochs = 1
[2022-09-13 15:06:57] - INFO: ### nums_labels = 6
[2022-09-13 15:06:57] - INFO: ### labels = ['cohesion', 'syntax', 'vocabulary', 'phraseology', 'grammar', 'conventions']
[2022-09-13 15:06:57] - INFO: ### return_dict = True
[2022-09-13 15:06:57] - INFO: ### output_hidden_states = False
[2022-09-13 15:06:57] - INFO: ### output_attentions = False
[2022-09-13 15:06:57] - INFO: ### torchscript = False
[2022-09-13 15:06:57] - INFO: ### torch_dtype = None
[2022-09-13 15:06:57] - INFO: ### use_bfloat16 = False
[2022-09-13 15:06:57] - INFO: ### pruned_heads = {}
[2022-09-13 15:06:57] - INFO: ### tie_word_embeddings = True
[2022-09-13 15:06:57] - INFO: ### is_encoder_decoder = False
[2022-09-13 15:06:57] - INFO: ### is_decoder = False
[2022-09-13 15:06:57] - INFO: ### cross_attention_hidden_size = None
[2022-09-13 15:06:57] - INFO: ### add_cross_attention = False
[2022-09-13 15:06:57] - INFO: ### tie_encoder_decoder = False
[2022-09-13 15:06:57] - INFO: ### max_length = 20
[2022-09-13 15:06:57] - INFO: ### min_length = 0
[2022-09-13 15:06:57] - INFO: ### do_sample = False
[2022-09-13 15:06:57] - INFO: ### early_stopping = False
[2022-09-13 15:06:57] - INFO: ### num_beams = 1
[2022-09-13 15:06:57] - INFO: ### num_beam_groups = 1
[2022-09-13 15:06:57] - INFO: ### diversity_penalty = 0.0
[2022-09-13 15:06:57] - INFO: ### temperature = 1.0
[2022-09-13 15:06:57] - INFO: ### top_k = 50
[2022-09-13 15:06:57] - INFO: ### top_p = 1.0
[2022-09-13 15:06:57] - INFO: ### typical_p = 1.0
[2022-09-13 15:06:57] - INFO: ### repetition_penalty = 1.0
[2022-09-13 15:06:57] - INFO: ### length_penalty = 1.0
[2022-09-13 15:06:57] - INFO: ### no_repeat_ngram_size = 0
[2022-09-13 15:06:57] - INFO: ### encoder_no_repeat_ngram_size = 0
[2022-09-13 15:06:57] - INFO: ### bad_words_ids = None
[2022-09-13 15:06:57] - INFO: ### num_return_sequences = 1
[2022-09-13 15:06:57] - INFO: ### chunk_size_feed_forward = 0
[2022-09-13 15:06:57] - INFO: ### output_scores = False
[2022-09-13 15:06:57] - INFO: ### return_dict_in_generate = False
[2022-09-13 15:06:57] - INFO: ### forced_bos_token_id = None
[2022-09-13 15:06:57] - INFO: ### forced_eos_token_id = None
[2022-09-13 15:06:57] - INFO: ### remove_invalid_values = False
[2022-09-13 15:06:57] - INFO: ### exponential_decay_length_penalty = None
[2022-09-13 15:06:57] - INFO: ### architectures = ['BertForMaskedLM']
[2022-09-13 15:06:57] - INFO: ### finetuning_task = None
[2022-09-13 15:06:57] - INFO: ### id2label = {0: 'LABEL_0', 1: 'LABEL_1'}
[2022-09-13 15:06:57] - INFO: ### label2id = {'LABEL_0': 0, 'LABEL_1': 1}
[2022-09-13 15:06:57] - INFO: ### tokenizer_class = None
[2022-09-13 15:06:57] - INFO: ### prefix = None
[2022-09-13 15:06:57] - INFO: ### bos_token_id = None
[2022-09-13 15:06:57] - INFO: ### pad_token_id = 0
[2022-09-13 15:06:57] - INFO: ### eos_token_id = None
[2022-09-13 15:06:57] - INFO: ### sep_token_id = None
[2022-09-13 15:06:57] - INFO: ### decoder_start_token_id = None
[2022-09-13 15:06:57] - INFO: ### task_specific_params = None
[2022-09-13 15:06:57] - INFO: ### problem_type = None
[2022-09-13 15:06:57] - INFO: ### _name_or_path = 
[2022-09-13 15:06:57] - INFO: ### transformers_version = 4.6.0.dev0
[2022-09-13 15:06:57] - INFO: ### gradient_checkpointing = False
[2022-09-13 15:06:57] - INFO: ### model_type = bert
[2022-09-13 15:06:57] - INFO: ### vocab_size = 30522
[2022-09-13 15:06:57] - INFO: ### hidden_size = 768
[2022-09-13 15:06:57] - INFO: ### num_hidden_layers = 12
[2022-09-13 15:06:57] - INFO: ### num_attention_heads = 12
[2022-09-13 15:06:57] - INFO: ### hidden_act = gelu
[2022-09-13 15:06:57] - INFO: ### intermediate_size = 3072
[2022-09-13 15:06:57] - INFO: ### hidden_dropout_prob = 0.1
[2022-09-13 15:06:57] - INFO: ### attention_probs_dropout_prob = 0.1
[2022-09-13 15:06:57] - INFO: ### max_position_embeddings = 512
[2022-09-13 15:06:57] - INFO: ### type_vocab_size = 2
[2022-09-13 15:06:57] - INFO: ### initializer_range = 0.02
[2022-09-13 15:06:57] - INFO: ### layer_norm_eps = 1e-12
[2022-09-13 15:06:57] - INFO: ### position_embedding_type = absolute
[2022-09-13 15:06:57] - INFO: ### use_cache = True
[2022-09-13 15:06:57] - INFO: ### classifier_dropout = None
[2022-09-13 15:06:58] - INFO:         text_id  ... conventions
0  0016926B079C  ...         3.0
1  0022683E9EA5  ...         2.5
2  00299B378633  ...         2.5
3  003885A45F42  ...         5.0
4  0049B1DF5CCC  ...         2.5

[5 rows x 8 columns]
[2022-09-13 19:53:37] - INFO:  ### 将当前配置打印到日志文件中 
[2022-09-13 19:53:37] - INFO: ### project_dir = /Users/liangyan/PycharmProjects/ELL
[2022-09-13 19:53:37] - INFO: ### dataset_dir = /Users/liangyan/PycharmProjects/ELL/data
[2022-09-13 19:53:37] - INFO: ### pretrained_model_dir = /Users/liangyan/PycharmProjects/ELL/bert_base_uncased
[2022-09-13 19:53:37] - INFO: ### vocab_path = /Users/liangyan/PycharmProjects/ELL/bert_base_uncased/vocab.txt
[2022-09-13 19:53:37] - INFO: ### device = cpu
[2022-09-13 19:53:37] - INFO: ### train_file_path = /Users/liangyan/PycharmProjects/ELL/data/train.csv
[2022-09-13 19:53:37] - INFO: ### test_file_path = /Users/liangyan/PycharmProjects/ELL/data/test.csv
[2022-09-13 19:53:37] - INFO: ### sub_file_path = /Users/liangyan/PycharmProjects/ELL/data/sample_submission.csv
[2022-09-13 19:53:37] - INFO: ### model_save_dir = /Users/liangyan/PycharmProjects/ELL/cache
[2022-09-13 19:53:37] - INFO: ### logs_save_dir = /Users/liangyan/PycharmProjects/ELL/logs
[2022-09-13 19:53:37] - INFO: ### batch_size = 20
[2022-09-13 19:53:37] - INFO: ### learning_rate = 3.5e-05
[2022-09-13 19:53:37] - INFO: ### epochs = 1
[2022-09-13 19:53:37] - INFO: ### nums_labels = 6
[2022-09-13 19:53:37] - INFO: ### labels = ['cohesion', 'syntax', 'vocabulary', 'phraseology', 'grammar', 'conventions']
[2022-09-13 19:53:37] - INFO: ### return_dict = True
[2022-09-13 19:53:37] - INFO: ### output_hidden_states = False
[2022-09-13 19:53:37] - INFO: ### output_attentions = False
[2022-09-13 19:53:37] - INFO: ### torchscript = False
[2022-09-13 19:53:37] - INFO: ### torch_dtype = None
[2022-09-13 19:53:37] - INFO: ### use_bfloat16 = False
[2022-09-13 19:53:37] - INFO: ### pruned_heads = {}
[2022-09-13 19:53:37] - INFO: ### tie_word_embeddings = True
[2022-09-13 19:53:37] - INFO: ### is_encoder_decoder = False
[2022-09-13 19:53:37] - INFO: ### is_decoder = False
[2022-09-13 19:53:37] - INFO: ### cross_attention_hidden_size = None
[2022-09-13 19:53:37] - INFO: ### add_cross_attention = False
[2022-09-13 19:53:37] - INFO: ### tie_encoder_decoder = False
[2022-09-13 19:53:37] - INFO: ### max_length = 20
[2022-09-13 19:53:37] - INFO: ### min_length = 0
[2022-09-13 19:53:37] - INFO: ### do_sample = False
[2022-09-13 19:53:37] - INFO: ### early_stopping = False
[2022-09-13 19:53:37] - INFO: ### num_beams = 1
[2022-09-13 19:53:37] - INFO: ### num_beam_groups = 1
[2022-09-13 19:53:37] - INFO: ### diversity_penalty = 0.0
[2022-09-13 19:53:37] - INFO: ### temperature = 1.0
[2022-09-13 19:53:37] - INFO: ### top_k = 50
[2022-09-13 19:53:37] - INFO: ### top_p = 1.0
[2022-09-13 19:53:37] - INFO: ### typical_p = 1.0
[2022-09-13 19:53:37] - INFO: ### repetition_penalty = 1.0
[2022-09-13 19:53:37] - INFO: ### length_penalty = 1.0
[2022-09-13 19:53:37] - INFO: ### no_repeat_ngram_size = 0
[2022-09-13 19:53:37] - INFO: ### encoder_no_repeat_ngram_size = 0
[2022-09-13 19:53:37] - INFO: ### bad_words_ids = None
[2022-09-13 19:53:37] - INFO: ### num_return_sequences = 1
[2022-09-13 19:53:37] - INFO: ### chunk_size_feed_forward = 0
[2022-09-13 19:53:37] - INFO: ### output_scores = False
[2022-09-13 19:53:37] - INFO: ### return_dict_in_generate = False
[2022-09-13 19:53:37] - INFO: ### forced_bos_token_id = None
[2022-09-13 19:53:37] - INFO: ### forced_eos_token_id = None
[2022-09-13 19:53:37] - INFO: ### remove_invalid_values = False
[2022-09-13 19:53:37] - INFO: ### exponential_decay_length_penalty = None
[2022-09-13 19:53:37] - INFO: ### architectures = ['BertForMaskedLM']
[2022-09-13 19:53:37] - INFO: ### finetuning_task = None
[2022-09-13 19:53:37] - INFO: ### id2label = {0: 'LABEL_0', 1: 'LABEL_1'}
[2022-09-13 19:53:37] - INFO: ### label2id = {'LABEL_0': 0, 'LABEL_1': 1}
[2022-09-13 19:53:37] - INFO: ### tokenizer_class = None
[2022-09-13 19:53:37] - INFO: ### prefix = None
[2022-09-13 19:53:37] - INFO: ### bos_token_id = None
[2022-09-13 19:53:37] - INFO: ### pad_token_id = 0
[2022-09-13 19:53:37] - INFO: ### eos_token_id = None
[2022-09-13 19:53:37] - INFO: ### sep_token_id = None
[2022-09-13 19:53:37] - INFO: ### decoder_start_token_id = None
[2022-09-13 19:53:37] - INFO: ### task_specific_params = None
[2022-09-13 19:53:37] - INFO: ### problem_type = None
[2022-09-13 19:53:37] - INFO: ### _name_or_path = 
[2022-09-13 19:53:37] - INFO: ### transformers_version = 4.6.0.dev0
[2022-09-13 19:53:37] - INFO: ### gradient_checkpointing = False
[2022-09-13 19:53:37] - INFO: ### model_type = bert
[2022-09-13 19:53:37] - INFO: ### vocab_size = 30522
[2022-09-13 19:53:37] - INFO: ### hidden_size = 768
[2022-09-13 19:53:37] - INFO: ### num_hidden_layers = 12
[2022-09-13 19:53:37] - INFO: ### num_attention_heads = 12
[2022-09-13 19:53:37] - INFO: ### hidden_act = gelu
[2022-09-13 19:53:37] - INFO: ### intermediate_size = 3072
[2022-09-13 19:53:37] - INFO: ### hidden_dropout_prob = 0.1
[2022-09-13 19:53:37] - INFO: ### attention_probs_dropout_prob = 0.1
[2022-09-13 19:53:37] - INFO: ### max_position_embeddings = 512
[2022-09-13 19:53:37] - INFO: ### type_vocab_size = 2
[2022-09-13 19:53:37] - INFO: ### initializer_range = 0.02
[2022-09-13 19:53:37] - INFO: ### layer_norm_eps = 1e-12
[2022-09-13 19:53:37] - INFO: ### position_embedding_type = absolute
[2022-09-13 19:53:37] - INFO: ### use_cache = True
[2022-09-13 19:53:37] - INFO: ### classifier_dropout = None
[2022-09-13 19:53:57] - INFO:  ### 将当前配置打印到日志文件中 
[2022-09-13 19:53:57] - INFO: ### project_dir = /Users/liangyan/PycharmProjects/ELL
[2022-09-13 19:53:57] - INFO: ### dataset_dir = /Users/liangyan/PycharmProjects/ELL/data
[2022-09-13 19:53:57] - INFO: ### pretrained_model_dir = /Users/liangyan/PycharmProjects/ELL/bert_base_uncased
[2022-09-13 19:53:57] - INFO: ### vocab_path = /Users/liangyan/PycharmProjects/ELL/bert_base_uncased/vocab.txt
[2022-09-13 19:53:57] - INFO: ### device = cpu
[2022-09-13 19:53:57] - INFO: ### train_file_path = /Users/liangyan/PycharmProjects/ELL/data/train.csv
[2022-09-13 19:53:57] - INFO: ### test_file_path = /Users/liangyan/PycharmProjects/ELL/data/test.csv
[2022-09-13 19:53:57] - INFO: ### sub_file_path = /Users/liangyan/PycharmProjects/ELL/data/sample_submission.csv
[2022-09-13 19:53:57] - INFO: ### model_save_dir = /Users/liangyan/PycharmProjects/ELL/cache
[2022-09-13 19:53:57] - INFO: ### logs_save_dir = /Users/liangyan/PycharmProjects/ELL/logs
[2022-09-13 19:53:57] - INFO: ### batch_size = 20
[2022-09-13 19:53:57] - INFO: ### learning_rate = 3.5e-05
[2022-09-13 19:53:57] - INFO: ### epochs = 1
[2022-09-13 19:53:57] - INFO: ### nums_labels = 6
[2022-09-13 19:53:57] - INFO: ### labels = ['cohesion', 'syntax', 'vocabulary', 'phraseology', 'grammar', 'conventions']
[2022-09-13 19:53:57] - INFO: ### return_dict = True
[2022-09-13 19:53:57] - INFO: ### output_hidden_states = False
[2022-09-13 19:53:57] - INFO: ### output_attentions = False
[2022-09-13 19:53:57] - INFO: ### torchscript = False
[2022-09-13 19:53:57] - INFO: ### torch_dtype = None
[2022-09-13 19:53:57] - INFO: ### use_bfloat16 = False
[2022-09-13 19:53:57] - INFO: ### pruned_heads = {}
[2022-09-13 19:53:57] - INFO: ### tie_word_embeddings = True
[2022-09-13 19:53:57] - INFO: ### is_encoder_decoder = False
[2022-09-13 19:53:57] - INFO: ### is_decoder = False
[2022-09-13 19:53:57] - INFO: ### cross_attention_hidden_size = None
[2022-09-13 19:53:57] - INFO: ### add_cross_attention = False
[2022-09-13 19:53:57] - INFO: ### tie_encoder_decoder = False
[2022-09-13 19:53:57] - INFO: ### max_length = 20
[2022-09-13 19:53:57] - INFO: ### min_length = 0
[2022-09-13 19:53:57] - INFO: ### do_sample = False
[2022-09-13 19:53:57] - INFO: ### early_stopping = False
[2022-09-13 19:53:57] - INFO: ### num_beams = 1
[2022-09-13 19:53:57] - INFO: ### num_beam_groups = 1
[2022-09-13 19:53:57] - INFO: ### diversity_penalty = 0.0
[2022-09-13 19:53:57] - INFO: ### temperature = 1.0
[2022-09-13 19:53:57] - INFO: ### top_k = 50
[2022-09-13 19:53:57] - INFO: ### top_p = 1.0
[2022-09-13 19:53:57] - INFO: ### typical_p = 1.0
[2022-09-13 19:53:57] - INFO: ### repetition_penalty = 1.0
[2022-09-13 19:53:57] - INFO: ### length_penalty = 1.0
[2022-09-13 19:53:57] - INFO: ### no_repeat_ngram_size = 0
[2022-09-13 19:53:57] - INFO: ### encoder_no_repeat_ngram_size = 0
[2022-09-13 19:53:57] - INFO: ### bad_words_ids = None
[2022-09-13 19:53:57] - INFO: ### num_return_sequences = 1
[2022-09-13 19:53:57] - INFO: ### chunk_size_feed_forward = 0
[2022-09-13 19:53:57] - INFO: ### output_scores = False
[2022-09-13 19:53:57] - INFO: ### return_dict_in_generate = False
[2022-09-13 19:53:57] - INFO: ### forced_bos_token_id = None
[2022-09-13 19:53:57] - INFO: ### forced_eos_token_id = None
[2022-09-13 19:53:57] - INFO: ### remove_invalid_values = False
[2022-09-13 19:53:57] - INFO: ### exponential_decay_length_penalty = None
[2022-09-13 19:53:57] - INFO: ### architectures = ['BertForMaskedLM']
[2022-09-13 19:53:57] - INFO: ### finetuning_task = None
[2022-09-13 19:53:57] - INFO: ### id2label = {0: 'LABEL_0', 1: 'LABEL_1'}
[2022-09-13 19:53:57] - INFO: ### label2id = {'LABEL_0': 0, 'LABEL_1': 1}
[2022-09-13 19:53:57] - INFO: ### tokenizer_class = None
[2022-09-13 19:53:57] - INFO: ### prefix = None
[2022-09-13 19:53:57] - INFO: ### bos_token_id = None
[2022-09-13 19:53:57] - INFO: ### pad_token_id = 0
[2022-09-13 19:53:57] - INFO: ### eos_token_id = None
[2022-09-13 19:53:57] - INFO: ### sep_token_id = None
[2022-09-13 19:53:57] - INFO: ### decoder_start_token_id = None
[2022-09-13 19:53:57] - INFO: ### task_specific_params = None
[2022-09-13 19:53:57] - INFO: ### problem_type = None
[2022-09-13 19:53:57] - INFO: ### _name_or_path = 
[2022-09-13 19:53:57] - INFO: ### transformers_version = 4.6.0.dev0
[2022-09-13 19:53:57] - INFO: ### gradient_checkpointing = False
[2022-09-13 19:53:57] - INFO: ### model_type = bert
[2022-09-13 19:53:57] - INFO: ### vocab_size = 30522
[2022-09-13 19:53:57] - INFO: ### hidden_size = 768
[2022-09-13 19:53:57] - INFO: ### num_hidden_layers = 12
[2022-09-13 19:53:57] - INFO: ### num_attention_heads = 12
[2022-09-13 19:53:57] - INFO: ### hidden_act = gelu
[2022-09-13 19:53:57] - INFO: ### intermediate_size = 3072
[2022-09-13 19:53:57] - INFO: ### hidden_dropout_prob = 0.1
[2022-09-13 19:53:57] - INFO: ### attention_probs_dropout_prob = 0.1
[2022-09-13 19:53:57] - INFO: ### max_position_embeddings = 512
[2022-09-13 19:53:57] - INFO: ### type_vocab_size = 2
[2022-09-13 19:53:57] - INFO: ### initializer_range = 0.02
[2022-09-13 19:53:57] - INFO: ### layer_norm_eps = 1e-12
[2022-09-13 19:53:57] - INFO: ### position_embedding_type = absolute
[2022-09-13 19:53:57] - INFO: ### use_cache = True
[2022-09-13 19:53:57] - INFO: ### classifier_dropout = None
[2022-09-13 19:53:59] - INFO:         text_id  ... conventions
0  0016926B079C  ...         3.0
1  0022683E9EA5  ...         2.5
2  00299B378633  ...         2.5
3  003885A45F42  ...         5.0
4  0049B1DF5CCC  ...         2.5

[5 rows x 8 columns]
