[2022-09-14 08:58:36] - INFO:  ### 将当前配置打印到日志文件中 
[2022-09-14 08:58:36] - INFO: ### project_dir = /Users/liangyan/PycharmProjects/ELL
[2022-09-14 08:58:36] - INFO: ### dataset_dir = /Users/liangyan/PycharmProjects/ELL/data
[2022-09-14 08:58:36] - INFO: ### pretrained_model_dir = /Users/liangyan/PycharmProjects/ELL/bert_base_uncased
[2022-09-14 08:58:36] - INFO: ### vocab_path = /Users/liangyan/PycharmProjects/ELL/bert_base_uncased/vocab.txt
[2022-09-14 08:58:36] - INFO: ### device = cpu
[2022-09-14 08:58:36] - INFO: ### train_file_path = /Users/liangyan/PycharmProjects/ELL/data/train.csv
[2022-09-14 08:58:36] - INFO: ### test_file_path = /Users/liangyan/PycharmProjects/ELL/data/test.csv
[2022-09-14 08:58:36] - INFO: ### sub_file_path = /Users/liangyan/PycharmProjects/ELL/data/sample_submission.csv
[2022-09-14 08:58:36] - INFO: ### model_save_dir = /Users/liangyan/PycharmProjects/ELL/cache
[2022-09-14 08:58:36] - INFO: ### logs_save_dir = /Users/liangyan/PycharmProjects/ELL/logs
[2022-09-14 08:58:36] - INFO: ### batch_size = 20
[2022-09-14 08:58:36] - INFO: ### learning_rate = 3.5e-05
[2022-09-14 08:58:36] - INFO: ### epochs = 3
[2022-09-14 08:58:36] - INFO: ### nums_labels = 6
[2022-09-14 08:58:36] - INFO: ### labels = ['cohesion', 'syntax', 'vocabulary', 'phraseology', 'grammar', 'conventions']
[2022-09-14 08:58:36] - INFO: ### return_dict = True
[2022-09-14 08:58:36] - INFO: ### output_hidden_states = False
[2022-09-14 08:58:36] - INFO: ### output_attentions = False
[2022-09-14 08:58:36] - INFO: ### torchscript = False
[2022-09-14 08:58:36] - INFO: ### torch_dtype = None
[2022-09-14 08:58:36] - INFO: ### use_bfloat16 = False
[2022-09-14 08:58:36] - INFO: ### pruned_heads = {}
[2022-09-14 08:58:36] - INFO: ### tie_word_embeddings = True
[2022-09-14 08:58:36] - INFO: ### is_encoder_decoder = False
[2022-09-14 08:58:36] - INFO: ### is_decoder = False
[2022-09-14 08:58:36] - INFO: ### cross_attention_hidden_size = None
[2022-09-14 08:58:36] - INFO: ### add_cross_attention = False
[2022-09-14 08:58:36] - INFO: ### tie_encoder_decoder = False
[2022-09-14 08:58:36] - INFO: ### max_length = 20
[2022-09-14 08:58:36] - INFO: ### min_length = 0
[2022-09-14 08:58:36] - INFO: ### do_sample = False
[2022-09-14 08:58:36] - INFO: ### early_stopping = False
[2022-09-14 08:58:36] - INFO: ### num_beams = 1
[2022-09-14 08:58:36] - INFO: ### num_beam_groups = 1
[2022-09-14 08:58:36] - INFO: ### diversity_penalty = 0.0
[2022-09-14 08:58:36] - INFO: ### temperature = 1.0
[2022-09-14 08:58:36] - INFO: ### top_k = 50
[2022-09-14 08:58:36] - INFO: ### top_p = 1.0
[2022-09-14 08:58:36] - INFO: ### typical_p = 1.0
[2022-09-14 08:58:36] - INFO: ### repetition_penalty = 1.0
[2022-09-14 08:58:36] - INFO: ### length_penalty = 1.0
[2022-09-14 08:58:36] - INFO: ### no_repeat_ngram_size = 0
[2022-09-14 08:58:36] - INFO: ### encoder_no_repeat_ngram_size = 0
[2022-09-14 08:58:36] - INFO: ### bad_words_ids = None
[2022-09-14 08:58:36] - INFO: ### num_return_sequences = 1
[2022-09-14 08:58:36] - INFO: ### chunk_size_feed_forward = 0
[2022-09-14 08:58:36] - INFO: ### output_scores = False
[2022-09-14 08:58:36] - INFO: ### return_dict_in_generate = False
[2022-09-14 08:58:36] - INFO: ### forced_bos_token_id = None
[2022-09-14 08:58:36] - INFO: ### forced_eos_token_id = None
[2022-09-14 08:58:36] - INFO: ### remove_invalid_values = False
[2022-09-14 08:58:36] - INFO: ### exponential_decay_length_penalty = None
[2022-09-14 08:58:36] - INFO: ### architectures = ['BertForMaskedLM']
[2022-09-14 08:58:36] - INFO: ### finetuning_task = None
[2022-09-14 08:58:36] - INFO: ### id2label = {0: 'LABEL_0', 1: 'LABEL_1'}
[2022-09-14 08:58:36] - INFO: ### label2id = {'LABEL_0': 0, 'LABEL_1': 1}
[2022-09-14 08:58:36] - INFO: ### tokenizer_class = None
[2022-09-14 08:58:36] - INFO: ### prefix = None
[2022-09-14 08:58:36] - INFO: ### bos_token_id = None
[2022-09-14 08:58:36] - INFO: ### pad_token_id = 0
[2022-09-14 08:58:36] - INFO: ### eos_token_id = None
[2022-09-14 08:58:36] - INFO: ### sep_token_id = None
[2022-09-14 08:58:36] - INFO: ### decoder_start_token_id = None
[2022-09-14 08:58:36] - INFO: ### task_specific_params = None
[2022-09-14 08:58:36] - INFO: ### problem_type = None
[2022-09-14 08:58:36] - INFO: ### _name_or_path = 
[2022-09-14 08:58:36] - INFO: ### transformers_version = 4.6.0.dev0
[2022-09-14 08:58:36] - INFO: ### gradient_checkpointing = False
[2022-09-14 08:58:36] - INFO: ### model_type = bert
[2022-09-14 08:58:36] - INFO: ### vocab_size = 30522
[2022-09-14 08:58:36] - INFO: ### hidden_size = 768
[2022-09-14 08:58:36] - INFO: ### num_hidden_layers = 12
[2022-09-14 08:58:36] - INFO: ### num_attention_heads = 12
[2022-09-14 08:58:36] - INFO: ### hidden_act = gelu
[2022-09-14 08:58:36] - INFO: ### intermediate_size = 3072
[2022-09-14 08:58:36] - INFO: ### hidden_dropout_prob = 0.1
[2022-09-14 08:58:36] - INFO: ### attention_probs_dropout_prob = 0.1
[2022-09-14 08:58:36] - INFO: ### max_position_embeddings = 512
[2022-09-14 08:58:36] - INFO: ### type_vocab_size = 2
[2022-09-14 08:58:36] - INFO: ### initializer_range = 0.02
[2022-09-14 08:58:36] - INFO: ### layer_norm_eps = 1e-12
[2022-09-14 08:58:36] - INFO: ### position_embedding_type = absolute
[2022-09-14 08:58:36] - INFO: ### use_cache = True
[2022-09-14 08:58:36] - INFO: ### classifier_dropout = None
[2022-09-14 08:58:53] - INFO:  ### 将当前配置打印到日志文件中 
[2022-09-14 08:58:53] - INFO: ### project_dir = /Users/liangyan/PycharmProjects/ELL
[2022-09-14 08:58:53] - INFO: ### dataset_dir = /Users/liangyan/PycharmProjects/ELL/data
[2022-09-14 08:58:53] - INFO: ### pretrained_model_dir = /Users/liangyan/PycharmProjects/ELL/bert_base_uncased
[2022-09-14 08:58:53] - INFO: ### vocab_path = /Users/liangyan/PycharmProjects/ELL/bert_base_uncased/vocab.txt
[2022-09-14 08:58:53] - INFO: ### device = cpu
[2022-09-14 08:58:53] - INFO: ### train_file_path = /Users/liangyan/PycharmProjects/ELL/data/train.csv
[2022-09-14 08:58:53] - INFO: ### test_file_path = /Users/liangyan/PycharmProjects/ELL/data/test.csv
[2022-09-14 08:58:53] - INFO: ### sub_file_path = /Users/liangyan/PycharmProjects/ELL/data/sample_submission.csv
[2022-09-14 08:58:53] - INFO: ### model_save_dir = /Users/liangyan/PycharmProjects/ELL/cache
[2022-09-14 08:58:53] - INFO: ### logs_save_dir = /Users/liangyan/PycharmProjects/ELL/logs
[2022-09-14 08:58:53] - INFO: ### batch_size = 20
[2022-09-14 08:58:53] - INFO: ### learning_rate = 3.5e-05
[2022-09-14 08:58:53] - INFO: ### epochs = 3
[2022-09-14 08:58:53] - INFO: ### nums_labels = 6
[2022-09-14 08:58:53] - INFO: ### labels = ['cohesion', 'syntax', 'vocabulary', 'phraseology', 'grammar', 'conventions']
[2022-09-14 08:58:53] - INFO: ### return_dict = True
[2022-09-14 08:58:53] - INFO: ### output_hidden_states = False
[2022-09-14 08:58:53] - INFO: ### output_attentions = False
[2022-09-14 08:58:53] - INFO: ### torchscript = False
[2022-09-14 08:58:53] - INFO: ### torch_dtype = None
[2022-09-14 08:58:53] - INFO: ### use_bfloat16 = False
[2022-09-14 08:58:53] - INFO: ### pruned_heads = {}
[2022-09-14 08:58:53] - INFO: ### tie_word_embeddings = True
[2022-09-14 08:58:53] - INFO: ### is_encoder_decoder = False
[2022-09-14 08:58:53] - INFO: ### is_decoder = False
[2022-09-14 08:58:53] - INFO: ### cross_attention_hidden_size = None
[2022-09-14 08:58:53] - INFO: ### add_cross_attention = False
[2022-09-14 08:58:53] - INFO: ### tie_encoder_decoder = False
[2022-09-14 08:58:53] - INFO: ### max_length = 20
[2022-09-14 08:58:53] - INFO: ### min_length = 0
[2022-09-14 08:58:53] - INFO: ### do_sample = False
[2022-09-14 08:58:53] - INFO: ### early_stopping = False
[2022-09-14 08:58:53] - INFO: ### num_beams = 1
[2022-09-14 08:58:53] - INFO: ### num_beam_groups = 1
[2022-09-14 08:58:53] - INFO: ### diversity_penalty = 0.0
[2022-09-14 08:58:53] - INFO: ### temperature = 1.0
[2022-09-14 08:58:53] - INFO: ### top_k = 50
[2022-09-14 08:58:53] - INFO: ### top_p = 1.0
[2022-09-14 08:58:53] - INFO: ### typical_p = 1.0
[2022-09-14 08:58:53] - INFO: ### repetition_penalty = 1.0
[2022-09-14 08:58:53] - INFO: ### length_penalty = 1.0
[2022-09-14 08:58:53] - INFO: ### no_repeat_ngram_size = 0
[2022-09-14 08:58:53] - INFO: ### encoder_no_repeat_ngram_size = 0
[2022-09-14 08:58:53] - INFO: ### bad_words_ids = None
[2022-09-14 08:58:53] - INFO: ### num_return_sequences = 1
[2022-09-14 08:58:53] - INFO: ### chunk_size_feed_forward = 0
[2022-09-14 08:58:53] - INFO: ### output_scores = False
[2022-09-14 08:58:53] - INFO: ### return_dict_in_generate = False
[2022-09-14 08:58:53] - INFO: ### forced_bos_token_id = None
[2022-09-14 08:58:53] - INFO: ### forced_eos_token_id = None
[2022-09-14 08:58:53] - INFO: ### remove_invalid_values = False
[2022-09-14 08:58:53] - INFO: ### exponential_decay_length_penalty = None
[2022-09-14 08:58:53] - INFO: ### architectures = ['BertForMaskedLM']
[2022-09-14 08:58:53] - INFO: ### finetuning_task = None
[2022-09-14 08:58:53] - INFO: ### id2label = {0: 'LABEL_0', 1: 'LABEL_1'}
[2022-09-14 08:58:53] - INFO: ### label2id = {'LABEL_0': 0, 'LABEL_1': 1}
[2022-09-14 08:58:53] - INFO: ### tokenizer_class = None
[2022-09-14 08:58:53] - INFO: ### prefix = None
[2022-09-14 08:58:53] - INFO: ### bos_token_id = None
[2022-09-14 08:58:53] - INFO: ### pad_token_id = 0
[2022-09-14 08:58:53] - INFO: ### eos_token_id = None
[2022-09-14 08:58:53] - INFO: ### sep_token_id = None
[2022-09-14 08:58:53] - INFO: ### decoder_start_token_id = None
[2022-09-14 08:58:53] - INFO: ### task_specific_params = None
[2022-09-14 08:58:53] - INFO: ### problem_type = None
[2022-09-14 08:58:53] - INFO: ### _name_or_path = 
[2022-09-14 08:58:53] - INFO: ### transformers_version = 4.6.0.dev0
[2022-09-14 08:58:53] - INFO: ### gradient_checkpointing = False
[2022-09-14 08:58:53] - INFO: ### model_type = bert
[2022-09-14 08:58:53] - INFO: ### vocab_size = 30522
[2022-09-14 08:58:53] - INFO: ### hidden_size = 768
[2022-09-14 08:58:53] - INFO: ### num_hidden_layers = 12
[2022-09-14 08:58:53] - INFO: ### num_attention_heads = 12
[2022-09-14 08:58:53] - INFO: ### hidden_act = gelu
[2022-09-14 08:58:53] - INFO: ### intermediate_size = 3072
[2022-09-14 08:58:53] - INFO: ### hidden_dropout_prob = 0.1
[2022-09-14 08:58:53] - INFO: ### attention_probs_dropout_prob = 0.1
[2022-09-14 08:58:53] - INFO: ### max_position_embeddings = 512
[2022-09-14 08:58:53] - INFO: ### type_vocab_size = 2
[2022-09-14 08:58:53] - INFO: ### initializer_range = 0.02
[2022-09-14 08:58:53] - INFO: ### layer_norm_eps = 1e-12
[2022-09-14 08:58:53] - INFO: ### position_embedding_type = absolute
[2022-09-14 08:58:53] - INFO: ### use_cache = True
[2022-09-14 08:58:53] - INFO: ### classifier_dropout = None
[2022-09-14 08:59:18] - INFO:  ### 将当前配置打印到日志文件中 
[2022-09-14 08:59:18] - INFO: ### project_dir = /Users/liangyan/PycharmProjects/ELL
[2022-09-14 08:59:18] - INFO: ### dataset_dir = /Users/liangyan/PycharmProjects/ELL/data
[2022-09-14 08:59:18] - INFO: ### pretrained_model_dir = /Users/liangyan/PycharmProjects/ELL/bert_base_uncased
[2022-09-14 08:59:18] - INFO: ### vocab_path = /Users/liangyan/PycharmProjects/ELL/bert_base_uncased/vocab.txt
[2022-09-14 08:59:18] - INFO: ### device = cpu
[2022-09-14 08:59:18] - INFO: ### train_file_path = /Users/liangyan/PycharmProjects/ELL/data/train.csv
[2022-09-14 08:59:18] - INFO: ### test_file_path = /Users/liangyan/PycharmProjects/ELL/data/test.csv
[2022-09-14 08:59:18] - INFO: ### sub_file_path = /Users/liangyan/PycharmProjects/ELL/data/sample_submission.csv
[2022-09-14 08:59:18] - INFO: ### model_save_dir = /Users/liangyan/PycharmProjects/ELL/cache
[2022-09-14 08:59:18] - INFO: ### logs_save_dir = /Users/liangyan/PycharmProjects/ELL/logs
[2022-09-14 08:59:18] - INFO: ### batch_size = 20
[2022-09-14 08:59:18] - INFO: ### learning_rate = 3.5e-05
[2022-09-14 08:59:18] - INFO: ### epochs = 3
[2022-09-14 08:59:18] - INFO: ### nums_labels = 6
[2022-09-14 08:59:18] - INFO: ### labels = ['cohesion', 'syntax', 'vocabulary', 'phraseology', 'grammar', 'conventions']
[2022-09-14 08:59:18] - INFO: ### return_dict = True
[2022-09-14 08:59:18] - INFO: ### output_hidden_states = False
[2022-09-14 08:59:18] - INFO: ### output_attentions = False
[2022-09-14 08:59:18] - INFO: ### torchscript = False
[2022-09-14 08:59:18] - INFO: ### torch_dtype = None
[2022-09-14 08:59:18] - INFO: ### use_bfloat16 = False
[2022-09-14 08:59:18] - INFO: ### pruned_heads = {}
[2022-09-14 08:59:18] - INFO: ### tie_word_embeddings = True
[2022-09-14 08:59:18] - INFO: ### is_encoder_decoder = False
[2022-09-14 08:59:18] - INFO: ### is_decoder = False
[2022-09-14 08:59:18] - INFO: ### cross_attention_hidden_size = None
[2022-09-14 08:59:18] - INFO: ### add_cross_attention = False
[2022-09-14 08:59:18] - INFO: ### tie_encoder_decoder = False
[2022-09-14 08:59:18] - INFO: ### max_length = 20
[2022-09-14 08:59:18] - INFO: ### min_length = 0
[2022-09-14 08:59:18] - INFO: ### do_sample = False
[2022-09-14 08:59:18] - INFO: ### early_stopping = False
[2022-09-14 08:59:18] - INFO: ### num_beams = 1
[2022-09-14 08:59:18] - INFO: ### num_beam_groups = 1
[2022-09-14 08:59:18] - INFO: ### diversity_penalty = 0.0
[2022-09-14 08:59:18] - INFO: ### temperature = 1.0
[2022-09-14 08:59:18] - INFO: ### top_k = 50
[2022-09-14 08:59:18] - INFO: ### top_p = 1.0
[2022-09-14 08:59:18] - INFO: ### typical_p = 1.0
[2022-09-14 08:59:18] - INFO: ### repetition_penalty = 1.0
[2022-09-14 08:59:18] - INFO: ### length_penalty = 1.0
[2022-09-14 08:59:18] - INFO: ### no_repeat_ngram_size = 0
[2022-09-14 08:59:18] - INFO: ### encoder_no_repeat_ngram_size = 0
[2022-09-14 08:59:18] - INFO: ### bad_words_ids = None
[2022-09-14 08:59:18] - INFO: ### num_return_sequences = 1
[2022-09-14 08:59:18] - INFO: ### chunk_size_feed_forward = 0
[2022-09-14 08:59:18] - INFO: ### output_scores = False
[2022-09-14 08:59:18] - INFO: ### return_dict_in_generate = False
[2022-09-14 08:59:18] - INFO: ### forced_bos_token_id = None
[2022-09-14 08:59:18] - INFO: ### forced_eos_token_id = None
[2022-09-14 08:59:18] - INFO: ### remove_invalid_values = False
[2022-09-14 08:59:18] - INFO: ### exponential_decay_length_penalty = None
[2022-09-14 08:59:18] - INFO: ### architectures = ['BertForMaskedLM']
[2022-09-14 08:59:18] - INFO: ### finetuning_task = None
[2022-09-14 08:59:18] - INFO: ### id2label = {0: 'LABEL_0', 1: 'LABEL_1'}
[2022-09-14 08:59:18] - INFO: ### label2id = {'LABEL_0': 0, 'LABEL_1': 1}
[2022-09-14 08:59:18] - INFO: ### tokenizer_class = None
[2022-09-14 08:59:18] - INFO: ### prefix = None
[2022-09-14 08:59:18] - INFO: ### bos_token_id = None
[2022-09-14 08:59:18] - INFO: ### pad_token_id = 0
[2022-09-14 08:59:18] - INFO: ### eos_token_id = None
[2022-09-14 08:59:18] - INFO: ### sep_token_id = None
[2022-09-14 08:59:18] - INFO: ### decoder_start_token_id = None
[2022-09-14 08:59:18] - INFO: ### task_specific_params = None
[2022-09-14 08:59:18] - INFO: ### problem_type = None
[2022-09-14 08:59:18] - INFO: ### _name_or_path = 
[2022-09-14 08:59:18] - INFO: ### transformers_version = 4.6.0.dev0
[2022-09-14 08:59:18] - INFO: ### gradient_checkpointing = False
[2022-09-14 08:59:18] - INFO: ### model_type = bert
[2022-09-14 08:59:18] - INFO: ### vocab_size = 30522
[2022-09-14 08:59:18] - INFO: ### hidden_size = 768
[2022-09-14 08:59:18] - INFO: ### num_hidden_layers = 12
[2022-09-14 08:59:18] - INFO: ### num_attention_heads = 12
[2022-09-14 08:59:18] - INFO: ### hidden_act = gelu
[2022-09-14 08:59:18] - INFO: ### intermediate_size = 3072
[2022-09-14 08:59:18] - INFO: ### hidden_dropout_prob = 0.1
[2022-09-14 08:59:18] - INFO: ### attention_probs_dropout_prob = 0.1
[2022-09-14 08:59:18] - INFO: ### max_position_embeddings = 512
[2022-09-14 08:59:18] - INFO: ### type_vocab_size = 2
[2022-09-14 08:59:18] - INFO: ### initializer_range = 0.02
[2022-09-14 08:59:18] - INFO: ### layer_norm_eps = 1e-12
[2022-09-14 08:59:18] - INFO: ### position_embedding_type = absolute
[2022-09-14 08:59:18] - INFO: ### use_cache = True
[2022-09-14 08:59:18] - INFO: ### classifier_dropout = None
[2022-09-14 08:59:18] - INFO: 0       I think that students would benefit from learn...
1       When a problem is a change you have to let it ...
2       Dear, Principal\n\nIf u change the school poli...
3       The best time in life is when you become yours...
4       Small act of kindness can impact in other peop...
                              ...                        
3906    I believe using cellphones in class for educat...
3907    Working alone, students do not have to argue w...
3908    "A problem is a chance for you to do your best...
3909    Many people disagree with Albert Schweitzer's ...
3910    Do you think that failure is the main thing fo...
Name: full_text, Length: 3911, dtype: object
